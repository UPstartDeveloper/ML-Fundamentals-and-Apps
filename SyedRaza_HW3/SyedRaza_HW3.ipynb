{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Custom Implementation\n",
    "\n",
    "Let's begin with the dataset given in the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x1   x2  y_true\n",
       "0  4.0  2.9       1\n",
       "1  4.0  4.0       1\n",
       "2  1.0  2.5      -1\n",
       "3  2.5  1.0      -1\n",
       "4  4.9  4.5       1\n",
       "5  1.9  1.9      -1\n",
       "6  3.5  4.0       1\n",
       "7  0.5  1.5      -1\n",
       "8  2.0  2.1      -1\n",
       "9  4.5  2.5       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat1 = np.array([4, 4, 1, 2.5, 4.9, 1.9, 3.5, 0.5, 2, 4.5])\n",
    "feat2 = np.array([2.9, 4, 2.5, 1, 4.5, 1.9, 4, 1.5, 2.1, 2.5])\n",
    "labels = np.array([1, 1, -1, -1, 1, -1, 1, -1, -1, 1])\n",
    "df = pd.DataFrame(data=dict({\n",
    "    \"x1\": feat1,\n",
    "    \"x2\": feat2,\n",
    "    \"y_true\": labels \n",
    "}))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now's the time to make this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def sigmoid(x: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Given an array of activation values, we return \n",
    "    an array of probabilities between 0-1. \n",
    "\n",
    "    They will NOT necessarily add up to 1.\n",
    "\n",
    "    This is mainly intended for classification problems.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BinaryClassificationMLP:\n",
    "    num_layers: int\n",
    "    units_per_layer: np.ndarray\n",
    "    num_features: int = None\n",
    "    threshold: float = 0.5\n",
    "    classes: np.ndarray = np.array([0, 1])\n",
    "\n",
    "    def define_model(self, num_features: int) -> None:\n",
    "        '''Glorot weight initialization'''\n",
    "        self.layers = list()\n",
    "        fan_in = self.num_features\n",
    "        fan_out_index = 0\n",
    "        for layer_index in range(self.num_layers):\n",
    "            # initialize using a randomly sampled uniform distribution\n",
    "            fan_out = self.units_per_layer[fan_out_index]\n",
    "            scale = max(1.0, (fan_in + fan_out) / 2.0)\n",
    "            limit = math.sqrt(3.0 * scale)\n",
    "            layer_weights = rng.uniform(low=-limit, high=limit, size=(fan_in, fan_out))\n",
    "            # biases - initialize to zeros, b/c we don't need to break symmetry (unlike for the weights)\n",
    "            layer_bias = np.zeros((fan_out, 1))\n",
    "            # activation - use tanh for hidden layers, and sigmoid for the last one\n",
    "            activation = np.tanh\n",
    "            if layer_index == self.num_layers - 1:\n",
    "                activation = sigmoid\n",
    "            # add to the list, and prep for next iteration\n",
    "            self.layers.append((layer_weights, layer_bias, activation))\n",
    "            fan_in = fan_out\n",
    "            fan_out_index += 1\n",
    "\n",
    "    def forward(self, X):\n",
    "        current_activation = X\n",
    "        layer_activations = list()\n",
    "        for weight, bias, act_func in self.layers:\n",
    "            layer_summation = current_activation @ weight + bias.T\n",
    "            current_activation = act_func(layer_summation)\n",
    "            layer_activations.append(current_activation)\n",
    "        return layer_activations\n",
    "\n",
    "    def backward(self, X, y, learning_rate, activations):\n",
    "        # variables we're going to need\n",
    "        weights1, bias1, act1 = self.layers[0]\n",
    "        weights2, bias2, act2 = self.layers[1]\n",
    "        per_sample_factor = (1 / X.shape[0])\n",
    "        hidden_layer_activation, output_layer_activations = (\n",
    "            activations[0], activations[1]\n",
    "        )\n",
    "        output_layer_weights = weights2\n",
    "        num_samples = X.shape[0]\n",
    "\n",
    "        # derivatives for the output layer\n",
    "        y_pred = output_layer_activations\n",
    "        y_true = np.where(y == -1, 0, 1).reshape(-1, 1)  # labels should be only 0/1\n",
    "        error = y_pred - y_true\n",
    "        derivative_y_pred = y_pred * (1 - y_pred)\n",
    "        grad_output_layer = error * derivative_y_pred\n",
    "        derivative_output_layer = dW2 = (grad_output_layer.T @ hidden_layer_activation).T\n",
    "        derivative_output_bias = db2 = (1 / num_samples) * np.sum(error, axis=1, keepdims=True)\n",
    "\n",
    "        # update weights in output layer before going fwd\n",
    "        new_output_weights = weights2 - learning_rate * dW2\n",
    "        new_output_bias = bias2 - learning_rate * bias2\n",
    "\n",
    "        # derivatives for the hidden layer\n",
    "        derivative_hidden_activation = z_prime = dZ1 = hidden_layer_activation * (1 - hidden_layer_activation)\n",
    "        grad_hidden_layer = grad_output_layer @ new_output_weights.T * z_prime\n",
    "        derivative_hidden_weights = X.T @ grad_hidden_layer\n",
    "        derivative_hidden_bias = db1 = (1 / num_samples) * np.sum(z_prime, axis=1, keepdims=True)\n",
    "\n",
    "        # update weights in the hidden layer\n",
    "        new_hidden_weights = weights1 - learning_rate * derivative_hidden_weights\n",
    "        new_hidden_bias = bias1 - learning_rate * bias1\n",
    "\n",
    "        # update the state of the model\n",
    "        self.layers[0] = (new_hidden_weights, new_hidden_bias, act1)\n",
    "        self.layers[1] = (new_output_weights, new_output_bias, act2)\n",
    "\n",
    "    def fit(self, X_train: np.array, y_train: np.array,\n",
    "            epochs=1000, learning_rate=0.0001) -> None:\n",
    "\n",
    "        # A: initial state of the network\n",
    "        num_features = X_train.shape[1]\n",
    "        self.define_model(num_features)\n",
    "\n",
    "        layer1, layer2 = self.layers[0], self.layers[1]\n",
    "\n",
    "        # B: training!\n",
    "        for _ in range(epochs):\n",
    "            activations = self.forward(X_train)\n",
    "            self.backward(X_train, y_train, learning_rate, activations)\n",
    "\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        activations = self.forward(X)\n",
    "        class1, class2 = self.classes\n",
    "        y_pred = np.where(activations[1] >= self.threshold, class2, class1)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this model using our specific hyperparams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "units_per_layer = [5, 1]\n",
    "num_features = 2\n",
    "classes = np.array([-1, 1])\n",
    "custom_mlp = BinaryClassificationMLP(\n",
    "    num_layers, units_per_layer, num_features, classes=classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data is half the fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[[\"x1\", \"x2\"]].values, df[\"y_true\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mlp.fit(X_train_scaled, y_train, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = accuracy_score(y_test, custom_mlp.predict(X_test_scaled))\n",
    "print(f\"Accuracy: {accuracy1 * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: `sklearn` Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/559/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sklearn_mlp = MLPClassifier(random_state=42, max_iter=300).fit(X_train_scaled, y_train)\n",
    "accuracy2 = sklearn_mlp.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy: {accuracy2 * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: A New Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from Custom: -1\n",
      "Prediction from Scikit-learn: 1\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([3, 3]).reshape(1, -1)\n",
    "y_preds = [model.predict(X_new).squeeze() for model in [custom_mlp, sklearn_mlp]]\n",
    "\n",
    "print(f\"Prediction from Custom: {y_preds[0]}\")\n",
    "print(f\"Prediction from Scikit-learn: {y_preds[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "559",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "814950ec8b8c7032d23fac38b059fcb51ad391395255f22f9a4e55af65449f0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
