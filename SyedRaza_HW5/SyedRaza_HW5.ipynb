{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102a0758",
   "metadata": {},
   "source": [
    "## <center> CS559 Machine Learning: Fundamentals and Applications</center>\n",
    "<center> Fall 2022 HW5</center>\n",
    "<center>Due: 12/16/2022 Friday 11:59 PM </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e2cb96",
   "metadata": {},
   "source": [
    "Homework assignments will be done individually: each student must hand in their own answers. Use of partial or entire solutions obtained from others or online is strictly prohibited. Electronic submission on Canvas is mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11037a17",
   "metadata": {},
   "source": [
    "# Graphical Models (30 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce17fb",
   "metadata": {},
   "source": [
    "Consider following tables:\n",
    "\n",
    "let C = random Bernoulli var\n",
    "\n",
    "||p(C)|\n",
    "|----|----|\n",
    "|F|0.65|\n",
    "|T|0.35|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491f1e5",
   "metadata": {},
   "source": [
    "let R and S also = random Bernoulli vars, conditional on C\n",
    "\n",
    "|C|p(S=F)|p(S=T)|p(R=F)|p(R=T)|\n",
    "|--|--|--|--|--|\n",
    "|F|0.7|0.3|0.8|0.2|\n",
    "|T|0.8|0.2|0.2|0.8|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8519c7",
   "metadata": {},
   "source": [
    "let W = random Bernoulli vars, conditional on R and S\n",
    "\n",
    "|S|R|p(W=T)|p(W=F)|\n",
    "|-|-|---|---|\n",
    "|F|F|0.96|0.04|\n",
    "|T|F|0.15|0.85|\n",
    "|F|T|0.17|0.83|\n",
    "|T|T|0.02|0.98|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8acf9cb8",
   "metadata": {},
   "source": [
    "## Part A\n",
    "(5 pts) Using pgmpy, draw a conditional directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6fafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "from daft import PGM\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pgmpy.models import BayesianNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb32324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pgm_to_pgmpy(pgm):\n",
    "    \"\"\"\n",
    "    Takes a Daft PGM object and converts it to a pgmpy BayesianModel.\n",
    "\n",
    "    (Function taken from class notebook).\n",
    "    \"\"\"\n",
    "    edges = [(edge.node1.name, edge.node2.name) for edge in pgm._edges]\n",
    "    model = BayesianNetwork(edges)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e2a992c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 314.961x314.961 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pgm = PGM(shape=[4, 4])\n",
    "\n",
    "pgm.add_node(daft.Node('C', r\"C\", 2, 4))\n",
    "pgm.add_node(daft.Node('R', r\"R\", 1, 2.5))\n",
    "pgm.add_node(daft.Node('S', r\"S\", 3, 2.5))\n",
    "pgm.add_node(daft.Node('W', r\"W\", 2, 1))\n",
    "\n",
    "\n",
    "pgm.add_edge('C', 'R')\n",
    "pgm.add_edge('C', 'S')\n",
    "pgm.add_edge('S', 'W')\n",
    "pgm.add_edge('R', 'W')\n",
    "\n",
    "pgm.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c514edd",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "(5 pts) Calculate a probability p(C,S,R,W).\n",
    "\n",
    "$$\\begin{equation}\\begin{split}\n",
    "\\sum_{S,R\\in\\{T,F\\}}p(C,S,R,W)&=\\sum_{S,R\\in\\{T,F\\}}p(C=T|S,R)p(S,R|W)p(W)\\\\\n",
    "&=p(C=T|S=T,R=T)p(S=T|R=T)p(W=T)\\\\\n",
    "&\\,\\,+p(C=T|S=T,R=F)p(S=T|R=F)p(W=T)\\\\\n",
    "&\\,\\,+p(C=T|S=F,R=T)p(S=F|R=T)p(W=T)\\\\\n",
    "&\\,\\,+p(C=T|S=F,R=F)p(S=F|R=F)p(W=T)\\\\\n",
    "\\end{split}\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4903cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 27.16%.\n"
     ]
    }
   ],
   "source": [
    "joint_probabilities = [\n",
    "    (0.2 * 0.8 * 0.02),  # P_CtStRtWt\n",
    "    (0.2 * 0.2 * 0.15),  # P_CtStRfWt\n",
    "    (0.8 * 0.8 * 0.17),  # P_CtSfRtWt\n",
    "    (0.8 * 0.2 * 0.96),  # P_CtSfRfWt\n",
    "    # (0.2 * 0.8 * 0.98),  # P_CtStRtWf\n",
    "    # (0.2 * 0.2 * 0.85),  # P_CtStRfWf\n",
    "    # (0.8 * 0.8 * 0.83),  # P_CtSfRtWf\n",
    "    # (0.8 * 0.2 * 0.04),  # P_CtSfRfWf\n",
    "]\n",
    "print(f\"Probability: {round(sum(joint_probabilities) * 100, 4)}%.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f7b47",
   "metadata": {},
   "source": [
    "## Part C\n",
    "(5 pts) Calculate a probability p(S=T|W=T).\n",
    "\n",
    "$$\\begin{equation}\\begin{split}\n",
    "P(S=T|W=T)&=\\frac{P(W=T|S=T)P(S=T)}{P(W=T)}\\\\\n",
    "&=\\frac{\\sum_{C\\in\\{T,F\\}}P(W=T,R,S=T)}{\\sum_{S,R\\in\\{T,F\\}}P(W=T,S,R)}\n",
    "\\end{split}\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f6d259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 6.5385%.\n"
     ]
    }
   ],
   "source": [
    "# numerator\n",
    "P_WtStRt = 0.02\n",
    "P_WtStRf = 0.15\n",
    "P_WtSt = P_WtStRt + P_WtStRf\n",
    "\n",
    "P_StCt = 0.3\n",
    "P_StCf = 0.2\n",
    "P_St = P_StCt + P_StCf\n",
    "\n",
    "# denominator\n",
    "P_WtSfRt = 0.17\n",
    "P_WtSfRf = 0.96\n",
    "P_Wt = P_WtStRt + P_WtStRf + P_WtSfRt + P_WtSfRf\n",
    "\n",
    "# tying it all together!\n",
    "P_StWt = (P_WtSt * P_St) / (P_Wt)\n",
    "P_StWt\n",
    "print(f\"Probability: {round(P_StWt * 100, 4)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8767335",
   "metadata": {},
   "source": [
    "## Part D\n",
    "(10 pts) Using a provided data table, `GM_train.csv`, train a graphical model using `pgmpy.models.BayesianModel` and report the accuracy. The target is W. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c615eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ecff2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>S</th>\n",
       "      <th>R</th>\n",
       "      <th>W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C  S  R  W\n",
       "0  0  1  1  0\n",
       "1  0  1  1  0\n",
       "2  1  1  0  0\n",
       "3  0  1  1  0\n",
       "4  1  0  0  1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"./GM_train.csv\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa1165",
   "metadata": {},
   "source": [
    "Training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e464cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------------------+--------------------+\n",
      "| R    | R(0) | R(0) | R(1)                | R(1)               |\n",
      "+------+------+------+---------------------+--------------------+\n",
      "| S    | S(0) | S(1) | S(0)                | S(1)               |\n",
      "+------+------+------+---------------------+--------------------+\n",
      "| W(0) | 0.56 | 0.65 | 0.46153846153846156 | 0.5517241379310345 |\n",
      "+------+------+------+---------------------+--------------------+\n",
      "| W(1) | 0.44 | 0.35 | 0.5384615384615384  | 0.4482758620689655 |\n",
      "+------+------+------+---------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "model = convert_pgm_to_pgmpy(pgm)\n",
    "model.fit(data_train)\n",
    "print(model.get_cpds(\"W\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be37783",
   "metadata": {},
   "source": [
    "Finally, let's get the training accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c8a9448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cfe14e896f4f26b5b7f70503095863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true = data_train[\"W\"]\n",
    "y_pred = model.predict(data_train.drop(\"W\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93fba21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 57.0%.\n"
     ]
    }
   ],
   "source": [
    "train_acc = metrics.accuracy_score(y_true, y_pred)\n",
    "print(f\"Train Accuracy: {round(train_acc * 100, 4)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babf17c",
   "metadata": {},
   "source": [
    "## Part E\n",
    "(5 pts) Generalize the model trained in **d** using `GM_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7098c217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819af477c34e409c90e4848fad010f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.0%.\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv(\"./GM_test.csv\")\n",
    "y_test = data_test[\"W\"]\n",
    "y_pred = model.predict(data_test.drop(\"W\", axis=1))\n",
    "test_acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {round(test_acc * 100, 4)}%.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5fdb6e7",
   "metadata": {},
   "source": [
    "# Handwriting recognition: (70 pts)\n",
    "Handwriting recognition is a well-studied subject in computer vision and has found wide applications in our daily life (such as USPS mail sorting). In this project, we will explore various machine learning techniques for recognizing handwriting digits. The dataset you will be using is the well-known MINST dataset.\n",
    "\n",
    "(1)\tThe MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. (http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "(2)\tBelow is an example of some digits from the MNIST dataset.\n",
    "\n",
    "![Image of MINST](https://datasets.activeloop.ai/wp-content/uploads/2019/12/MNIST-handwritten-digits-dataset-visualized-by-Activeloop.webp)\n",
    "\n",
    "(3)\tThe goal of this assignment is to build a 10-class classifier to recognize those handwriting digits as accurately as you can.  All the assignments below should use the training data (60K examples) and test data (10 K examples) as given by the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7115b7b9",
   "metadata": {},
   "source": [
    "## Part A\n",
    "(30 pts) Build several non-deep learning based classifiers using all pixels as features for handwriting recognition. You need to use at least **three techniques** we have learned from the class to do the work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146daff",
   "metadata": {},
   "source": [
    "### Step 1: Load and Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c67e2c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./mnist-in-csv/mnist_train.csv\")\n",
    "\n",
    "df_test = pd.read_csv(\"./mnist-in-csv/mnist_test.csv\")\n",
    "\n",
    "display(df_train.head())\n",
    "columns = df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81780d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = df_train.drop(\"label\", axis=1), df_test.drop(\"label\", axis=1)\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "\n",
    "y_train, y_test = (\n",
    "    df_train[\"label\"].values,\n",
    "    df_test[\"label\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d5617",
   "metadata": {},
   "source": [
    "### Step 2: Evaluate Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9873497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72936f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/559/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Applications/anaconda3/envs/559/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Applications/anaconda3/envs/559/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>Training</th>\n",
       "      <th>Testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.981917</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.926200</td>\n",
       "      <td>0.9183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.856900</td>\n",
       "      <td>0.8603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_type  Training  Testing\n",
       "0  Nearest Neighbors  0.981917   0.9688\n",
       "1         Linear SVM  0.926200   0.9183\n",
       "2      Decision Tree  0.856900   0.8603"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# define list of models to try\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"Decision Tree\",\n",
    "]\n",
    "# instantiate them\n",
    "regressors = [\n",
    "    KNeighborsClassifier(5),\n",
    "    LinearSVC(random_state=42, C=1.0),  # note: C can be decreased to strength regularization\n",
    "    RandomForestClassifier(max_depth=5),\n",
    "]\n",
    "\n",
    "# init a dict of scoring for each model\n",
    "model_results = []\n",
    "\n",
    "# apply each model\n",
    "for name, model in zip(names, regressors):\n",
    "    # record the score\n",
    "    model.fit(X_train, y_train)\n",
    "    train_accuracy = metrics.accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    model_results.append(pd.Series({\n",
    "        \"model_type\": name,\n",
    "        \"Training\": train_accuracy, \n",
    "        \"Testing\": test_accuracy,\n",
    "    }))\n",
    "# take a look at the result\n",
    "df = pd.DataFrame(model_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b98827",
   "metadata": {},
   "source": [
    "b.\t(10 pts) In this assignment, we will explore various techniques related to a neural network with hidden layers of more than 3 to solve the 10-class classification problem.\n",
    "\n",
    "Since there are many existing implementations to solve the MINST problem, we need to give some twists to this problem to make it worthwhile to do for our final assignment. (Please refer to the ranking list for MNIST at [http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html).\n",
    "\n",
    "The basic network structure that we are trying to explore is something like the following (i.e., the fully connected deep neural nets). The number of hidden layers and the size of each hidden layer in terms of neurons are left as tuning parameters that you can explore.\n",
    "\n",
    "![Image of NN](https://pimages.toolbox.com/wp-content/uploads/2022/05/18113202/The-Architecture-of-a-Neural-Network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8a08f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "# CNN and MLP architecture\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Rescaling,\n",
    ")\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomNormal\n",
    "# Keras Callbacks\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "# Image Preprocessing\n",
    "from PIL import Image\n",
    "# Optimizing Hyperparameters\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "# Loading Model from JSON file\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08711d77",
   "metadata": {},
   "source": [
    "Now I'll reload the dataset, but in a Tensorflow-y way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "98d21c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "319dd7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df_train = df_train[\"label\"]\n",
    "y_df_train = LabelBinarizer().fit_transform(y_df_train)\n",
    "y_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "404666de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df_test = df_test[\"label\"]\n",
    "y_df_test = LabelBinarizer().fit_transform(y_df_test)\n",
    "y_df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1659dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_train = df_train.drop(\"label\", axis=1) / 255.\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(tf.expand_dims(\n",
    "                # np.array([np.newaxis, X_df_train.values]),\n",
    "                X_df_train.values.reshape(-1, 784),\n",
    "            1), tf.float32),\n",
    "            tf.cast(y_df_train.reshape(-1, 1, 10), tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# optimizes the efficiency of loading training data\n",
    "train_ds_batches = (\n",
    "      train_ds.cache().shuffle(500)\n",
    "      .prefetch(tf.data.AUTOTUNE)).batch(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9cb5f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_test = df_test.drop(\"label\", axis=1) / 255.\n",
    "\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(tf.expand_dims(\n",
    "                # X_df_test.values.reshape(-1, 784),\n",
    "                X_df_test.values.reshape(-1, 784),\n",
    "            1), tf.float32),\n",
    "            tf.cast(y_df_test.reshape(-1, 1, 10), tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# optimizes the efficiency of loading training data\n",
    "test_ds_batches = (\n",
    "      test_ds.cache().shuffle(500)\n",
    "      .prefetch(tf.data.AUTOTUNE)).batch(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182ce18",
   "metadata": {},
   "source": [
    "Let's create a validation set (to make the training easier to monitor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b69e396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 14:47:49.828266: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2022-12-04 14:47:49.912252: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    }
   ],
   "source": [
    "def is_validation(image, _):\n",
    "    '''marks every 10th sample a part of the validation data'''\n",
    "    return image % 10 == 0\n",
    "\n",
    "def is_not_validation(image, _):\n",
    "    '''puts everything back in the train set, that should be there'''\n",
    "    return not is_validation(image, _)\n",
    "\n",
    "\n",
    "def recover_label(_, label):\n",
    "    return label\n",
    "\n",
    "\n",
    "val_ds = (\n",
    "    train_ds_batches.enumerate().filter(is_validation) \\\n",
    "    .map(recover_label)\n",
    ")\n",
    "\n",
    "train_ds_batches = (\n",
    "    train_ds_batches.enumerate().filter(is_not_validation) \\\n",
    "    .map(recover_label)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0323f67a",
   "metadata": {},
   "source": [
    "Let's add some helper functions to make the code less repetitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "1de1b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to Reduce Repetition\n",
    "def add_conv_layer(model, layer_size, needs_input, kernel_size=None, pool_size=None):\n",
    "    \"\"\"Add a Keras convolutional layer to the model, along with MaxPooling.\n",
    "       Will specify input shape as well if needed.\n",
    "       \n",
    "       Parameters:\n",
    "       model(Model): Neural network in Keras\n",
    "       layer_size(int): number of neurons to go in layer\n",
    "       need_input(bool): signals if the convolutional layer needs to specify\n",
    "                         the dimensions of the input\n",
    "       kernel_size(tuple): specifies a square matrix to use for kernel dimensions\n",
    "       pool_size(tuple): specifies a square matrix to use in pooling\n",
    "       \n",
    "       Returns: None\n",
    "       \n",
    "    \"\"\"\n",
    "    # set kernel and pool size\n",
    "    if kernel_size is None:\n",
    "        kernel_size = (3, 3)\n",
    "    if pool_size is None:\n",
    "        pool_size = (2, 2)\n",
    "    # specify input dimension for 1st conv layer\n",
    "    if needs_input is True:\n",
    "        conv_layer = Conv2D(layer_size,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='relu',\n",
    "                            input_shape=(...))\n",
    "\n",
    "    else:\n",
    "        # otherwise all other convolutional layers don't need it\n",
    "        conv_layer = Conv2D(layer_size,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='relu')\n",
    "    # add Convolutional layer\n",
    "    model.add(conv_layer)  \n",
    "    # add MaxPooling layer\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))  # no learning params\n",
    "    return None\n",
    "\n",
    "\n",
    "def add_dense_layer(model, layer_size, is_output, drop_rate):\n",
    "    \"\"\"Add a multi-layer perceptron to the model\n",
    "       Will specify 'sigmoid' for the final layer.\n",
    "       \n",
    "       Parameters:\n",
    "       model(Model): Neural network in Keras\n",
    "       layer_size(int): number of neurons to go in layer\n",
    "       is_output(bdool): signals if the MLP is the last layer\n",
    "       drop_rate(float): percentage of connections in Dense layer\n",
    "                       to cut off\n",
    "       \n",
    "       Returns: None\n",
    "       \n",
    "    \"\"\"\n",
    "    # specify activation function\n",
    "    activation = 'relu' if is_output is False else 'softmax'\n",
    "    # add MLP\n",
    "    model.add(Dense(layer_size, activation=activation)) \n",
    "    # Add Dropout layer and Batch Normalization\n",
    "    # else:\n",
    "    if is_output is False:\n",
    "        # model.add(Dense(layer_size, activation=activation)) \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(drop_rate))\n",
    "    return None\n",
    "\n",
    "\n",
    "def compile_model(model, optimizer=None):\n",
    "    \"\"\"Compile the neural network.\n",
    "    \n",
    "       Parameter:\n",
    "       model(keras.Sequential or keras.Model): the model object\n",
    "       optimizer(str): specfies the algoritm used to minimize loss\n",
    "       \n",
    "       Returns: None\n",
    "       \n",
    "    \"\"\"\n",
    "    # set the optimizer\n",
    "    if optimizer is None:\n",
    "        optimizer = 'adam'\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy',\n",
    "                           tf.keras.metrics.Precision(),\n",
    "                           tf.keras.metrics.Recall()])\n",
    "    return None\n",
    "\n",
    "\n",
    "def define_model(units, conv_layers, dense_layers, dropout, num_classes=10):\n",
    "    \"\"\"Define a Sequential model in Keras.\n",
    "    \n",
    "       Parameters:\n",
    "       units(int): number of neurons to go in a layer\n",
    "       conv_layers(int): number of convolutional layers\n",
    "       dense_layers(int): number of MLP\n",
    "       dropout(float): percentage of connections in Dense layer\n",
    "                       to cut off\n",
    "                       \n",
    "       Returns: tf.keras.Sequential: the neural network to train\n",
    "    \n",
    "    \"\"\"\n",
    "    # Instaniate model\n",
    "    model = Sequential()\n",
    "    # Add CNN layers\n",
    "    if conv_layers > 0:\n",
    "        add_conv_layer(model, units, True)\n",
    "        for _ in range(conv_layers - 1):\n",
    "            # add convolutional layers that come after the 1st\n",
    "            add_conv_layer(model, units, False)\n",
    "    # Flatten the data\n",
    "    if conv_layers > 0:\n",
    "        model.add(Flatten())\n",
    "    else:  # if no flattening, tell the model what input shape to expect\n",
    "        model.add(keras.layers.InputLayer(input_shape=(1, 784)))\n",
    "    # Add MLP Layers\n",
    "    for _ in range(dense_layers - 1):\n",
    "        add_dense_layer(model, units, False, dropout)\n",
    "    # add final MLP, for output\n",
    "    add_dense_layer(model, num_classes, True, dropout)\n",
    "    # Compile Model\n",
    "    compile_model(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_ds, val_ds,\n",
    "                epochs, callbacks):\n",
    "    \"\"\"Train the Keras model.\n",
    "       \n",
    "       Parameters:\n",
    "       model(keras.Sequential or keras.Model): the model object\n",
    "       train_ds(tf.Dataset): subsection of the dataset for training\n",
    "       val_ds(tf.Dataset): subsection of the dataset for validation\n",
    "       epochs(int): number of forward and back passes for the entire\n",
    "                    dataset through the model\n",
    "       callbacks(List): special Keras functions to improve models\n",
    "       \n",
    "       returns: History.history: a dict containing metrics about model\n",
    "       \n",
    "    \"\"\"\n",
    "    # train the model\n",
    "    history = model.fit(x=train_ds,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_ds,\n",
    "                        callbacks=callbacks)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741db1de",
   "metadata": {},
   "source": [
    "Now, let's try out a few quick 'n dirty model  to finally get things moving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c80b7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0d41869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1080/1080 [==============================] - 9s 4ms/step - loss: 1.5117 - accuracy: 0.4761 - precision_57: 0.7620 - recall_57: 0.2440 - val_loss: 0.6270 - val_accuracy: 0.7717 - val_precision_57: 0.8452 - val_recall_57: 0.6587\n",
      "Epoch 2/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.9062 - accuracy: 0.6935 - precision_57: 0.8006 - recall_57: 0.5592 - val_loss: 0.4060 - val_accuracy: 0.8862 - val_precision_57: 0.9148 - val_recall_57: 0.8610\n",
      "Epoch 3/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.7447 - accuracy: 0.7721 - precision_57: 0.8340 - recall_57: 0.6932 - val_loss: 0.3761 - val_accuracy: 0.8957 - val_precision_57: 0.9117 - val_recall_57: 0.8862\n",
      "Epoch 4/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.6627 - accuracy: 0.8095 - precision_57: 0.8563 - recall_57: 0.7527 - val_loss: 0.3018 - val_accuracy: 0.9118 - val_precision_57: 0.9244 - val_recall_57: 0.9027\n",
      "Epoch 5/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.6252 - accuracy: 0.8224 - precision_57: 0.8674 - recall_57: 0.7723 - val_loss: 0.2856 - val_accuracy: 0.9202 - val_precision_57: 0.9304 - val_recall_57: 0.9133\n",
      "Epoch 6/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5971 - accuracy: 0.8348 - precision_57: 0.8772 - recall_57: 0.7879 - val_loss: 0.2810 - val_accuracy: 0.9200 - val_precision_57: 0.9307 - val_recall_57: 0.9127\n",
      "Epoch 7/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5690 - accuracy: 0.8414 - precision_57: 0.8816 - recall_57: 0.7982 - val_loss: 0.2536 - val_accuracy: 0.9295 - val_precision_57: 0.9384 - val_recall_57: 0.9245\n",
      "Epoch 8/20\n",
      "1080/1080 [==============================] - 3s 2ms/step - loss: 0.5516 - accuracy: 0.8483 - precision_57: 0.8857 - recall_57: 0.8066 - val_loss: 0.2628 - val_accuracy: 0.9280 - val_precision_57: 0.9397 - val_recall_57: 0.9223\n",
      "Epoch 9/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5389 - accuracy: 0.8518 - precision_57: 0.8879 - recall_57: 0.8117 - val_loss: 0.2370 - val_accuracy: 0.9362 - val_precision_57: 0.9429 - val_recall_57: 0.9307\n",
      "Epoch 10/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5308 - accuracy: 0.8546 - precision_57: 0.8918 - recall_57: 0.8162 - val_loss: 0.2347 - val_accuracy: 0.9357 - val_precision_57: 0.9434 - val_recall_57: 0.9312\n",
      "Epoch 11/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5216 - accuracy: 0.8587 - precision_57: 0.8945 - recall_57: 0.8193 - val_loss: 0.2172 - val_accuracy: 0.9357 - val_precision_57: 0.9437 - val_recall_57: 0.9300\n",
      "Epoch 12/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5121 - accuracy: 0.8612 - precision_57: 0.8960 - recall_57: 0.8234 - val_loss: 0.2269 - val_accuracy: 0.9365 - val_precision_57: 0.9442 - val_recall_57: 0.9315\n",
      "Epoch 13/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5100 - accuracy: 0.8624 - precision_57: 0.8979 - recall_57: 0.8264 - val_loss: 0.2242 - val_accuracy: 0.9387 - val_precision_57: 0.9463 - val_recall_57: 0.9342\n",
      "Epoch 14/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5037 - accuracy: 0.8648 - precision_57: 0.8982 - recall_57: 0.8287 - val_loss: 0.2272 - val_accuracy: 0.9398 - val_precision_57: 0.9475 - val_recall_57: 0.9352\n",
      "Epoch 15/20\n",
      "1080/1080 [==============================] - 3s 2ms/step - loss: 0.5035 - accuracy: 0.8631 - precision_57: 0.8983 - recall_57: 0.8272 - val_loss: 0.1994 - val_accuracy: 0.9433 - val_precision_57: 0.9505 - val_recall_57: 0.9383\n",
      "Epoch 16/20\n",
      "1080/1080 [==============================] - 3s 2ms/step - loss: 0.4918 - accuracy: 0.8663 - precision_57: 0.8998 - recall_57: 0.8314 - val_loss: 0.1699 - val_accuracy: 0.9528 - val_precision_57: 0.9592 - val_recall_57: 0.9480\n",
      "Epoch 17/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4924 - accuracy: 0.8670 - precision_57: 0.9015 - recall_57: 0.8336 - val_loss: 0.2141 - val_accuracy: 0.9357 - val_precision_57: 0.9457 - val_recall_57: 0.9297\n",
      "Epoch 18/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4821 - accuracy: 0.8699 - precision_57: 0.9027 - recall_57: 0.8357 - val_loss: 0.2297 - val_accuracy: 0.9393 - val_precision_57: 0.9460 - val_recall_57: 0.9352\n",
      "Epoch 19/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4785 - accuracy: 0.8709 - precision_57: 0.9016 - recall_57: 0.8367 - val_loss: 0.2081 - val_accuracy: 0.9432 - val_precision_57: 0.9490 - val_recall_57: 0.9400\n",
      "Epoch 20/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.4756 - accuracy: 0.8713 - precision_57: 0.9023 - recall_57: 0.8395 - val_loss: 0.1976 - val_accuracy: 0.9457 - val_precision_57: 0.9542 - val_recall_57: 0.9415\n",
      "Epoch 1/20\n",
      "1080/1080 [==============================] - 11s 5ms/step - loss: 1.0672 - accuracy: 0.6610 - precision_58: 0.8245 - recall_58: 0.5286 - val_loss: 0.3220 - val_accuracy: 0.9052 - val_precision_58: 0.9193 - val_recall_58: 0.8925\n",
      "Epoch 2/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.5133 - accuracy: 0.8572 - precision_58: 0.8914 - recall_58: 0.8231 - val_loss: 0.2559 - val_accuracy: 0.9278 - val_precision_58: 0.9395 - val_recall_58: 0.9207\n",
      "Epoch 3/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.4160 - accuracy: 0.8863 - precision_58: 0.9120 - recall_58: 0.8637 - val_loss: 0.1836 - val_accuracy: 0.9483 - val_precision_58: 0.9576 - val_recall_58: 0.9443\n",
      "Epoch 4/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.3712 - accuracy: 0.8992 - precision_58: 0.9215 - recall_58: 0.8802 - val_loss: 0.1827 - val_accuracy: 0.9480 - val_precision_58: 0.9535 - val_recall_58: 0.9432\n",
      "Epoch 5/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.3417 - accuracy: 0.9088 - precision_58: 0.9274 - recall_58: 0.8917 - val_loss: 0.1569 - val_accuracy: 0.9555 - val_precision_58: 0.9610 - val_recall_58: 0.9533\n",
      "Epoch 6/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.3127 - accuracy: 0.9150 - precision_58: 0.9326 - recall_58: 0.8997 - val_loss: 0.1493 - val_accuracy: 0.9565 - val_precision_58: 0.9616 - val_recall_58: 0.9513\n",
      "Epoch 7/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.3058 - accuracy: 0.9174 - precision_58: 0.9337 - recall_58: 0.9041 - val_loss: 0.1390 - val_accuracy: 0.9602 - val_precision_58: 0.9641 - val_recall_58: 0.9588\n",
      "Epoch 8/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.2881 - accuracy: 0.9237 - precision_58: 0.9386 - recall_58: 0.9103 - val_loss: 0.1345 - val_accuracy: 0.9632 - val_precision_58: 0.9665 - val_recall_58: 0.9607\n",
      "Epoch 9/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.2870 - accuracy: 0.9242 - precision_58: 0.9397 - recall_58: 0.9118 - val_loss: 0.1357 - val_accuracy: 0.9613 - val_precision_58: 0.9656 - val_recall_58: 0.9588\n",
      "Epoch 10/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.2731 - accuracy: 0.9278 - precision_58: 0.9426 - recall_58: 0.9151 - val_loss: 0.1421 - val_accuracy: 0.9622 - val_precision_58: 0.9661 - val_recall_58: 0.9598\n",
      "Epoch 11/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.2708 - accuracy: 0.9263 - precision_58: 0.9414 - recall_58: 0.9144 - val_loss: 0.1243 - val_accuracy: 0.9662 - val_precision_58: 0.9703 - val_recall_58: 0.9635\n",
      "Epoch 12/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.2682 - accuracy: 0.9273 - precision_58: 0.9420 - recall_58: 0.9161 - val_loss: 0.1153 - val_accuracy: 0.9692 - val_precision_58: 0.9725 - val_recall_58: 0.9672\n",
      "Epoch 13/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.2582 - accuracy: 0.9310 - precision_58: 0.9454 - recall_58: 0.9206 - val_loss: 0.1051 - val_accuracy: 0.9708 - val_precision_58: 0.9739 - val_recall_58: 0.9690\n",
      "Epoch 14/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.2521 - accuracy: 0.9327 - precision_58: 0.9462 - recall_58: 0.9216 - val_loss: 0.1010 - val_accuracy: 0.9727 - val_precision_58: 0.9760 - val_recall_58: 0.9693\n",
      "Epoch 15/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2479 - accuracy: 0.9337 - precision_58: 0.9467 - recall_58: 0.9228 - val_loss: 0.1107 - val_accuracy: 0.9702 - val_precision_58: 0.9732 - val_recall_58: 0.9678\n",
      "Epoch 16/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.2427 - accuracy: 0.9353 - precision_58: 0.9483 - recall_58: 0.9248 - val_loss: 0.1231 - val_accuracy: 0.9647 - val_precision_58: 0.9693 - val_recall_58: 0.9625\n",
      "Epoch 17/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.2340 - accuracy: 0.9362 - precision_58: 0.9489 - recall_58: 0.9257 - val_loss: 0.1170 - val_accuracy: 0.9658 - val_precision_58: 0.9689 - val_recall_58: 0.9645\n",
      "Epoch 18/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.2357 - accuracy: 0.9364 - precision_58: 0.9487 - recall_58: 0.9261 - val_loss: 0.1031 - val_accuracy: 0.9732 - val_precision_58: 0.9759 - val_recall_58: 0.9715\n",
      "Epoch 19/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2363 - accuracy: 0.9369 - precision_58: 0.9491 - recall_58: 0.9263 - val_loss: 0.0935 - val_accuracy: 0.9725 - val_precision_58: 0.9768 - val_recall_58: 0.9703\n",
      "Epoch 20/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.2313 - accuracy: 0.9377 - precision_58: 0.9499 - recall_58: 0.9277 - val_loss: 0.1026 - val_accuracy: 0.9705 - val_precision_58: 0.9739 - val_recall_58: 0.9687\n",
      "Epoch 1/20\n",
      "1080/1080 [==============================] - 9s 4ms/step - loss: 0.7490 - accuracy: 0.7686 - precision_59: 0.8583 - recall_59: 0.7018 - val_loss: 0.2344 - val_accuracy: 0.9335 - val_precision_59: 0.9430 - val_recall_59: 0.9238\n",
      "Epoch 2/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.3590 - accuracy: 0.8983 - precision_59: 0.9209 - recall_59: 0.8784 - val_loss: 0.1693 - val_accuracy: 0.9472 - val_precision_59: 0.9542 - val_recall_59: 0.9412\n",
      "Epoch 3/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2915 - accuracy: 0.9192 - precision_59: 0.9346 - recall_59: 0.9046 - val_loss: 0.1370 - val_accuracy: 0.9578 - val_precision_59: 0.9622 - val_recall_59: 0.9547\n",
      "Epoch 4/20\n",
      "1080/1080 [==============================] - 6s 5ms/step - loss: 0.2585 - accuracy: 0.9280 - precision_59: 0.9415 - recall_59: 0.9173 - val_loss: 0.1327 - val_accuracy: 0.9620 - val_precision_59: 0.9671 - val_recall_59: 0.9568\n",
      "Epoch 5/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.2288 - accuracy: 0.9349 - precision_59: 0.9472 - recall_59: 0.9251 - val_loss: 0.1177 - val_accuracy: 0.9640 - val_precision_59: 0.9676 - val_recall_59: 0.9615\n",
      "Epoch 6/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.2125 - accuracy: 0.9403 - precision_59: 0.9515 - recall_59: 0.9316 - val_loss: 0.1003 - val_accuracy: 0.9707 - val_precision_59: 0.9763 - val_recall_59: 0.9678\n",
      "Epoch 7/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.1994 - accuracy: 0.9425 - precision_59: 0.9533 - recall_59: 0.9347 - val_loss: 0.0856 - val_accuracy: 0.9755 - val_precision_59: 0.9789 - val_recall_59: 0.9728\n",
      "Epoch 8/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.1879 - accuracy: 0.9472 - precision_59: 0.9566 - recall_59: 0.9398 - val_loss: 0.0889 - val_accuracy: 0.9735 - val_precision_59: 0.9771 - val_recall_59: 0.9722\n",
      "Epoch 9/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.1846 - accuracy: 0.9484 - precision_59: 0.9579 - recall_59: 0.9415 - val_loss: 0.0811 - val_accuracy: 0.9758 - val_precision_59: 0.9794 - val_recall_59: 0.9728\n",
      "Epoch 10/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1754 - accuracy: 0.9515 - precision_59: 0.9600 - recall_59: 0.9444 - val_loss: 0.0815 - val_accuracy: 0.9767 - val_precision_59: 0.9778 - val_recall_59: 0.9752\n",
      "Epoch 11/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1688 - accuracy: 0.9523 - precision_59: 0.9605 - recall_59: 0.9462 - val_loss: 0.0817 - val_accuracy: 0.9772 - val_precision_59: 0.9792 - val_recall_59: 0.9750\n",
      "Epoch 12/20\n",
      "1080/1080 [==============================] - 8s 7ms/step - loss: 0.1622 - accuracy: 0.9539 - precision_59: 0.9623 - recall_59: 0.9482 - val_loss: 0.0661 - val_accuracy: 0.9790 - val_precision_59: 0.9823 - val_recall_59: 0.9778\n",
      "Epoch 13/20\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.1601 - accuracy: 0.9547 - precision_59: 0.9632 - recall_59: 0.9486 - val_loss: 0.0672 - val_accuracy: 0.9808 - val_precision_59: 0.9837 - val_recall_59: 0.9787\n",
      "Epoch 14/20\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 0.1551 - accuracy: 0.9556 - precision_59: 0.9627 - recall_59: 0.9487 - val_loss: 0.0661 - val_accuracy: 0.9810 - val_precision_59: 0.9843 - val_recall_59: 0.9795\n",
      "Epoch 15/20\n",
      "1080/1080 [==============================] - 6s 5ms/step - loss: 0.1493 - accuracy: 0.9574 - precision_59: 0.9643 - recall_59: 0.9518 - val_loss: 0.0675 - val_accuracy: 0.9803 - val_precision_59: 0.9821 - val_recall_59: 0.9780\n",
      "Epoch 16/20\n",
      "1080/1080 [==============================] - 6s 5ms/step - loss: 0.1474 - accuracy: 0.9585 - precision_59: 0.9648 - recall_59: 0.9531 - val_loss: 0.0514 - val_accuracy: 0.9857 - val_precision_59: 0.9880 - val_recall_59: 0.9842\n",
      "Epoch 17/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.1410 - accuracy: 0.9595 - precision_59: 0.9660 - recall_59: 0.9538 - val_loss: 0.0555 - val_accuracy: 0.9825 - val_precision_59: 0.9848 - val_recall_59: 0.9813\n",
      "Epoch 18/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.1395 - accuracy: 0.9610 - precision_59: 0.9673 - recall_59: 0.9564 - val_loss: 0.0587 - val_accuracy: 0.9818 - val_precision_59: 0.9844 - val_recall_59: 0.9805\n",
      "Epoch 19/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.1402 - accuracy: 0.9594 - precision_59: 0.9660 - recall_59: 0.9543 - val_loss: 0.0547 - val_accuracy: 0.9855 - val_precision_59: 0.9871 - val_recall_59: 0.9842\n",
      "Epoch 20/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.1324 - accuracy: 0.9617 - precision_59: 0.9680 - recall_59: 0.9569 - val_loss: 0.0517 - val_accuracy: 0.9858 - val_precision_59: 0.9876 - val_recall_59: 0.9848\n",
      "Epoch 1/20\n",
      "1080/1080 [==============================] - 12s 5ms/step - loss: 1.3357 - accuracy: 0.5596 - precision_60: 0.8195 - recall_60: 0.3507 - val_loss: 0.4861 - val_accuracy: 0.8555 - val_precision_60: 0.9182 - val_recall_60: 0.7803\n",
      "Epoch 2/20\n",
      "1080/1080 [==============================] - 6s 5ms/step - loss: 0.7680 - accuracy: 0.7631 - precision_60: 0.8479 - recall_60: 0.6653 - val_loss: 0.3380 - val_accuracy: 0.9058 - val_precision_60: 0.9301 - val_recall_60: 0.8852\n",
      "Epoch 3/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.6572 - accuracy: 0.8061 - precision_60: 0.8651 - recall_60: 0.7410 - val_loss: 0.2952 - val_accuracy: 0.9135 - val_precision_60: 0.9324 - val_recall_60: 0.8992\n",
      "Epoch 4/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.6050 - accuracy: 0.8268 - precision_60: 0.8768 - recall_60: 0.7713 - val_loss: 0.2856 - val_accuracy: 0.9233 - val_precision_60: 0.9393 - val_recall_60: 0.9137\n",
      "Epoch 5/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.5721 - accuracy: 0.8370 - precision_60: 0.8837 - recall_60: 0.7886 - val_loss: 0.2626 - val_accuracy: 0.9247 - val_precision_60: 0.9388 - val_recall_60: 0.9145\n",
      "Epoch 6/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.5552 - accuracy: 0.8438 - precision_60: 0.8876 - recall_60: 0.7979 - val_loss: 0.2523 - val_accuracy: 0.9310 - val_precision_60: 0.9435 - val_recall_60: 0.9210\n",
      "Epoch 7/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.5402 - accuracy: 0.8481 - precision_60: 0.8916 - recall_60: 0.8039 - val_loss: 0.2500 - val_accuracy: 0.9270 - val_precision_60: 0.9418 - val_recall_60: 0.9173\n",
      "Epoch 8/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.5203 - accuracy: 0.8544 - precision_60: 0.8949 - recall_60: 0.8137 - val_loss: 0.2456 - val_accuracy: 0.9298 - val_precision_60: 0.9432 - val_recall_60: 0.9195\n",
      "Epoch 9/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.5204 - accuracy: 0.8573 - precision_60: 0.8977 - recall_60: 0.8169 - val_loss: 0.2617 - val_accuracy: 0.9210 - val_precision_60: 0.9396 - val_recall_60: 0.9125\n",
      "Epoch 10/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.5110 - accuracy: 0.8577 - precision_60: 0.8980 - recall_60: 0.8180 - val_loss: 0.2219 - val_accuracy: 0.9377 - val_precision_60: 0.9508 - val_recall_60: 0.9280\n",
      "Epoch 11/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.5111 - accuracy: 0.8588 - precision_60: 0.8984 - recall_60: 0.8193 - val_loss: 0.2386 - val_accuracy: 0.9313 - val_precision_60: 0.9447 - val_recall_60: 0.9232\n",
      "Epoch 12/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.5136 - accuracy: 0.8586 - precision_60: 0.8973 - recall_60: 0.8191 - val_loss: 0.2496 - val_accuracy: 0.9305 - val_precision_60: 0.9423 - val_recall_60: 0.9220\n",
      "Epoch 13/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.4967 - accuracy: 0.8621 - precision_60: 0.8997 - recall_60: 0.8241 - val_loss: 0.2531 - val_accuracy: 0.9277 - val_precision_60: 0.9420 - val_recall_60: 0.9205\n",
      "Epoch 14/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4919 - accuracy: 0.8645 - precision_60: 0.9021 - recall_60: 0.8261 - val_loss: 0.2300 - val_accuracy: 0.9348 - val_precision_60: 0.9461 - val_recall_60: 0.9268\n",
      "Epoch 15/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4878 - accuracy: 0.8643 - precision_60: 0.9025 - recall_60: 0.8279 - val_loss: 0.2363 - val_accuracy: 0.9347 - val_precision_60: 0.9466 - val_recall_60: 0.9255\n",
      "Epoch 16/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4778 - accuracy: 0.8681 - precision_60: 0.9038 - recall_60: 0.8334 - val_loss: 0.2327 - val_accuracy: 0.9345 - val_precision_60: 0.9470 - val_recall_60: 0.9257\n",
      "Epoch 17/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4820 - accuracy: 0.8686 - precision_60: 0.9051 - recall_60: 0.8321 - val_loss: 0.2387 - val_accuracy: 0.9317 - val_precision_60: 0.9445 - val_recall_60: 0.9225\n",
      "Epoch 18/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.4632 - accuracy: 0.8728 - precision_60: 0.9081 - recall_60: 0.8371 - val_loss: 0.2363 - val_accuracy: 0.9325 - val_precision_60: 0.9454 - val_recall_60: 0.9243\n",
      "Epoch 19/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.4686 - accuracy: 0.8713 - precision_60: 0.9072 - recall_60: 0.8371 - val_loss: 0.2158 - val_accuracy: 0.9400 - val_precision_60: 0.9514 - val_recall_60: 0.9293\n",
      "Epoch 20/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.4642 - accuracy: 0.8716 - precision_60: 0.9067 - recall_60: 0.8381 - val_loss: 0.2093 - val_accuracy: 0.9425 - val_precision_60: 0.9547 - val_recall_60: 0.9347\n",
      "Epoch 1/20\n",
      "1080/1080 [==============================] - 13s 6ms/step - loss: 0.8555 - accuracy: 0.7349 - precision_61: 0.8580 - recall_61: 0.6286 - val_loss: 0.3037 - val_accuracy: 0.9083 - val_precision_61: 0.9240 - val_recall_61: 0.8958\n",
      "Epoch 2/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.4546 - accuracy: 0.8712 - precision_61: 0.9035 - recall_61: 0.8411 - val_loss: 0.2149 - val_accuracy: 0.9352 - val_precision_61: 0.9467 - val_recall_61: 0.9262\n",
      "Epoch 3/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.3939 - accuracy: 0.8887 - precision_61: 0.9146 - recall_61: 0.8661 - val_loss: 0.2030 - val_accuracy: 0.9405 - val_precision_61: 0.9490 - val_recall_61: 0.9362\n",
      "Epoch 4/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.3566 - accuracy: 0.8990 - precision_61: 0.9216 - recall_61: 0.8801 - val_loss: 0.1829 - val_accuracy: 0.9462 - val_precision_61: 0.9543 - val_recall_61: 0.9405\n",
      "Epoch 5/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.3281 - accuracy: 0.9083 - precision_61: 0.9284 - recall_61: 0.8906 - val_loss: 0.1769 - val_accuracy: 0.9483 - val_precision_61: 0.9546 - val_recall_61: 0.9435\n",
      "Epoch 6/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.3145 - accuracy: 0.9104 - precision_61: 0.9293 - recall_61: 0.8948 - val_loss: 0.1457 - val_accuracy: 0.9585 - val_precision_61: 0.9647 - val_recall_61: 0.9530\n",
      "Epoch 7/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.3010 - accuracy: 0.9157 - precision_61: 0.9329 - recall_61: 0.9006 - val_loss: 0.1587 - val_accuracy: 0.9500 - val_precision_61: 0.9557 - val_recall_61: 0.9453\n",
      "Epoch 8/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.2876 - accuracy: 0.9202 - precision_61: 0.9371 - recall_61: 0.9064 - val_loss: 0.1463 - val_accuracy: 0.9578 - val_precision_61: 0.9628 - val_recall_61: 0.9532\n",
      "Epoch 9/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.2795 - accuracy: 0.9217 - precision_61: 0.9380 - recall_61: 0.9089 - val_loss: 0.1513 - val_accuracy: 0.9558 - val_precision_61: 0.9637 - val_recall_61: 0.9517\n",
      "Epoch 10/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.2766 - accuracy: 0.9229 - precision_61: 0.9386 - recall_61: 0.9088 - val_loss: 0.1460 - val_accuracy: 0.9598 - val_precision_61: 0.9638 - val_recall_61: 0.9547\n",
      "Epoch 11/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.2643 - accuracy: 0.9259 - precision_61: 0.9411 - recall_61: 0.9139 - val_loss: 0.1478 - val_accuracy: 0.9570 - val_precision_61: 0.9623 - val_recall_61: 0.9530\n",
      "Epoch 12/20\n",
      "1080/1080 [==============================] - 6s 5ms/step - loss: 0.2670 - accuracy: 0.9263 - precision_61: 0.9411 - recall_61: 0.9133 - val_loss: 0.1211 - val_accuracy: 0.9637 - val_precision_61: 0.9687 - val_recall_61: 0.9590\n",
      "Epoch 13/20\n",
      "1080/1080 [==============================] - 6s 5ms/step - loss: 0.2562 - accuracy: 0.9287 - precision_61: 0.9437 - recall_61: 0.9162 - val_loss: 0.1394 - val_accuracy: 0.9597 - val_precision_61: 0.9644 - val_recall_61: 0.9565\n",
      "Epoch 14/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.2554 - accuracy: 0.9287 - precision_61: 0.9430 - recall_61: 0.9166 - val_loss: 0.1230 - val_accuracy: 0.9598 - val_precision_61: 0.9660 - val_recall_61: 0.9570\n",
      "Epoch 15/20\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 0.2513 - accuracy: 0.9298 - precision_61: 0.9446 - recall_61: 0.9176 - val_loss: 0.1125 - val_accuracy: 0.9657 - val_precision_61: 0.9711 - val_recall_61: 0.9618\n",
      "Epoch 16/20\n",
      "1080/1080 [==============================] - 6s 5ms/step - loss: 0.2419 - accuracy: 0.9312 - precision_61: 0.9454 - recall_61: 0.9196 - val_loss: 0.1187 - val_accuracy: 0.9650 - val_precision_61: 0.9690 - val_recall_61: 0.9600\n",
      "Epoch 17/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.2451 - accuracy: 0.9305 - precision_61: 0.9449 - recall_61: 0.9183 - val_loss: 0.1067 - val_accuracy: 0.9708 - val_precision_61: 0.9746 - val_recall_61: 0.9668\n",
      "Epoch 18/20\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 0.2399 - accuracy: 0.9329 - precision_61: 0.9459 - recall_61: 0.9214 - val_loss: 0.1098 - val_accuracy: 0.9663 - val_precision_61: 0.9714 - val_recall_61: 0.9623\n",
      "Epoch 19/20\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 0.2453 - accuracy: 0.9307 - precision_61: 0.9448 - recall_61: 0.9189 - val_loss: 0.1194 - val_accuracy: 0.9648 - val_precision_61: 0.9699 - val_recall_61: 0.9617\n",
      "Epoch 20/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.2328 - accuracy: 0.9334 - precision_61: 0.9473 - recall_61: 0.9238 - val_loss: 0.1169 - val_accuracy: 0.9655 - val_precision_61: 0.9697 - val_recall_61: 0.9613\n",
      "Epoch 1/20\n",
      "1080/1080 [==============================] - 18s 6ms/step - loss: 0.6068 - accuracy: 0.8128 - precision_62: 0.8822 - recall_62: 0.7605 - val_loss: 0.2215 - val_accuracy: 0.9320 - val_precision_62: 0.9456 - val_recall_62: 0.9240\n",
      "Epoch 2/20\n",
      "1080/1080 [==============================] - 8s 8ms/step - loss: 0.3129 - accuracy: 0.9089 - precision_62: 0.9276 - recall_62: 0.8922 - val_loss: 0.1501 - val_accuracy: 0.9545 - val_precision_62: 0.9616 - val_recall_62: 0.9482\n",
      "Epoch 3/20\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.2614 - accuracy: 0.9239 - precision_62: 0.9386 - recall_62: 0.9119 - val_loss: 0.1168 - val_accuracy: 0.9633 - val_precision_62: 0.9680 - val_recall_62: 0.9587\n",
      "Epoch 4/20\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.2292 - accuracy: 0.9325 - precision_62: 0.9455 - recall_62: 0.9225 - val_loss: 0.1217 - val_accuracy: 0.9642 - val_precision_62: 0.9694 - val_recall_62: 0.9598\n",
      "Epoch 5/20\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 0.2058 - accuracy: 0.9392 - precision_62: 0.9499 - recall_62: 0.9306 - val_loss: 0.1057 - val_accuracy: 0.9687 - val_precision_62: 0.9727 - val_recall_62: 0.9667\n",
      "Epoch 6/20\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 0.1938 - accuracy: 0.9436 - precision_62: 0.9538 - recall_62: 0.9358 - val_loss: 0.0968 - val_accuracy: 0.9698 - val_precision_62: 0.9732 - val_recall_62: 0.9675\n",
      "Epoch 7/20\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 0.1788 - accuracy: 0.9481 - precision_62: 0.9574 - recall_62: 0.9408 - val_loss: 0.0791 - val_accuracy: 0.9767 - val_precision_62: 0.9791 - val_recall_62: 0.9743\n",
      "Epoch 8/20\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 0.1708 - accuracy: 0.9505 - precision_62: 0.9591 - recall_62: 0.9439 - val_loss: 0.0751 - val_accuracy: 0.9770 - val_precision_62: 0.9805 - val_recall_62: 0.9747\n",
      "Epoch 9/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.1613 - accuracy: 0.9525 - precision_62: 0.9599 - recall_62: 0.9464 - val_loss: 0.0702 - val_accuracy: 0.9778 - val_precision_62: 0.9802 - val_recall_62: 0.9747\n",
      "Epoch 10/20\n",
      "1080/1080 [==============================] - 9s 8ms/step - loss: 0.1529 - accuracy: 0.9552 - precision_62: 0.9627 - recall_62: 0.9492 - val_loss: 0.0600 - val_accuracy: 0.9817 - val_precision_62: 0.9851 - val_recall_62: 0.9807\n",
      "Epoch 11/20\n",
      "1080/1080 [==============================] - 8s 7ms/step - loss: 0.1483 - accuracy: 0.9572 - precision_62: 0.9639 - recall_62: 0.9513 - val_loss: 0.0581 - val_accuracy: 0.9822 - val_precision_62: 0.9836 - val_recall_62: 0.9808\n",
      "Epoch 12/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.1406 - accuracy: 0.9585 - precision_62: 0.9653 - recall_62: 0.9532 - val_loss: 0.0560 - val_accuracy: 0.9827 - val_precision_62: 0.9854 - val_recall_62: 0.9815\n",
      "Epoch 13/20\n",
      "1080/1080 [==============================] - 6s 5ms/step - loss: 0.1414 - accuracy: 0.9588 - precision_62: 0.9655 - recall_62: 0.9539 - val_loss: 0.0566 - val_accuracy: 0.9833 - val_precision_62: 0.9855 - val_recall_62: 0.9822\n",
      "Epoch 14/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.1349 - accuracy: 0.9596 - precision_62: 0.9665 - recall_62: 0.9547 - val_loss: 0.0484 - val_accuracy: 0.9852 - val_precision_62: 0.9865 - val_recall_62: 0.9835\n",
      "Epoch 15/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.1307 - accuracy: 0.9614 - precision_62: 0.9676 - recall_62: 0.9563 - val_loss: 0.0522 - val_accuracy: 0.9838 - val_precision_62: 0.9861 - val_recall_62: 0.9828\n",
      "Epoch 16/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.1299 - accuracy: 0.9625 - precision_62: 0.9682 - recall_62: 0.9577 - val_loss: 0.0492 - val_accuracy: 0.9857 - val_precision_62: 0.9880 - val_recall_62: 0.9842\n",
      "Epoch 17/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.1226 - accuracy: 0.9644 - precision_62: 0.9702 - recall_62: 0.9595 - val_loss: 0.0510 - val_accuracy: 0.9845 - val_precision_62: 0.9864 - val_recall_62: 0.9818\n",
      "Epoch 18/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.1188 - accuracy: 0.9651 - precision_62: 0.9710 - recall_62: 0.9606 - val_loss: 0.0405 - val_accuracy: 0.9880 - val_precision_62: 0.9888 - val_recall_62: 0.9868\n",
      "Epoch 19/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.1189 - accuracy: 0.9651 - precision_62: 0.9702 - recall_62: 0.9605 - val_loss: 0.0483 - val_accuracy: 0.9860 - val_precision_62: 0.9891 - val_recall_62: 0.9843\n",
      "Epoch 20/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.1170 - accuracy: 0.9646 - precision_62: 0.9695 - recall_62: 0.9605 - val_loss: 0.0487 - val_accuracy: 0.9842 - val_precision_62: 0.9865 - val_recall_62: 0.9837\n",
      "Epoch 1/20\n",
      "1080/1080 [==============================] - 9s 5ms/step - loss: 1.0398 - accuracy: 0.6614 - precision_63: 0.8513 - recall_63: 0.4844 - val_loss: 0.3614 - val_accuracy: 0.8967 - val_precision_63: 0.9257 - val_recall_63: 0.8718\n",
      "Epoch 2/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.5961 - accuracy: 0.8234 - precision_63: 0.8796 - recall_63: 0.7635 - val_loss: 0.2722 - val_accuracy: 0.9183 - val_precision_63: 0.9415 - val_recall_63: 0.9018\n",
      "Epoch 3/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.5279 - accuracy: 0.8463 - precision_63: 0.8916 - recall_63: 0.8008 - val_loss: 0.2490 - val_accuracy: 0.9263 - val_precision_63: 0.9436 - val_recall_63: 0.9112\n",
      "Epoch 4/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.4962 - accuracy: 0.8551 - precision_63: 0.8979 - recall_63: 0.8157 - val_loss: 0.2555 - val_accuracy: 0.9267 - val_precision_63: 0.9433 - val_recall_63: 0.9142\n",
      "Epoch 5/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4718 - accuracy: 0.8651 - precision_63: 0.9029 - recall_63: 0.8274 - val_loss: 0.2466 - val_accuracy: 0.9288 - val_precision_63: 0.9450 - val_recall_63: 0.9160\n",
      "Epoch 6/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4555 - accuracy: 0.8684 - precision_63: 0.9067 - recall_63: 0.8339 - val_loss: 0.2318 - val_accuracy: 0.9340 - val_precision_63: 0.9496 - val_recall_63: 0.9240\n",
      "Epoch 7/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.4438 - accuracy: 0.8727 - precision_63: 0.9099 - recall_63: 0.8388 - val_loss: 0.2255 - val_accuracy: 0.9353 - val_precision_63: 0.9489 - val_recall_63: 0.9222\n",
      "Epoch 8/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4374 - accuracy: 0.8754 - precision_63: 0.9103 - recall_63: 0.8420 - val_loss: 0.2155 - val_accuracy: 0.9368 - val_precision_63: 0.9488 - val_recall_63: 0.9272\n",
      "Epoch 9/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.4261 - accuracy: 0.8772 - precision_63: 0.9116 - recall_63: 0.8446 - val_loss: 0.2102 - val_accuracy: 0.9423 - val_precision_63: 0.9518 - val_recall_63: 0.9322\n",
      "Epoch 10/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4202 - accuracy: 0.8796 - precision_63: 0.9126 - recall_63: 0.8469 - val_loss: 0.2065 - val_accuracy: 0.9400 - val_precision_63: 0.9479 - val_recall_63: 0.9285\n",
      "Epoch 11/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4195 - accuracy: 0.8785 - precision_63: 0.9123 - recall_63: 0.8485 - val_loss: 0.1947 - val_accuracy: 0.9417 - val_precision_63: 0.9555 - val_recall_63: 0.9330\n",
      "Epoch 12/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4185 - accuracy: 0.8790 - precision_63: 0.9131 - recall_63: 0.8479 - val_loss: 0.2101 - val_accuracy: 0.9388 - val_precision_63: 0.9507 - val_recall_63: 0.9287\n",
      "Epoch 13/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4117 - accuracy: 0.8835 - precision_63: 0.9156 - recall_63: 0.8521 - val_loss: 0.2204 - val_accuracy: 0.9388 - val_precision_63: 0.9493 - val_recall_63: 0.9305\n",
      "Epoch 14/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.4086 - accuracy: 0.8832 - precision_63: 0.9156 - recall_63: 0.8538 - val_loss: 0.2139 - val_accuracy: 0.9390 - val_precision_63: 0.9509 - val_recall_63: 0.9303\n",
      "Epoch 15/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4077 - accuracy: 0.8832 - precision_63: 0.9151 - recall_63: 0.8539 - val_loss: 0.1874 - val_accuracy: 0.9450 - val_precision_63: 0.9561 - val_recall_63: 0.9365\n",
      "Epoch 16/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.4067 - accuracy: 0.8835 - precision_63: 0.9153 - recall_63: 0.8544 - val_loss: 0.2171 - val_accuracy: 0.9375 - val_precision_63: 0.9542 - val_recall_63: 0.9268\n",
      "Epoch 17/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4012 - accuracy: 0.8840 - precision_63: 0.9159 - recall_63: 0.8559 - val_loss: 0.1984 - val_accuracy: 0.9420 - val_precision_63: 0.9547 - val_recall_63: 0.9338\n",
      "Epoch 18/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.3990 - accuracy: 0.8863 - precision_63: 0.9178 - recall_63: 0.8571 - val_loss: 0.1749 - val_accuracy: 0.9442 - val_precision_63: 0.9543 - val_recall_63: 0.9357\n",
      "Epoch 19/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4022 - accuracy: 0.8845 - precision_63: 0.9158 - recall_63: 0.8559 - val_loss: 0.2048 - val_accuracy: 0.9435 - val_precision_63: 0.9527 - val_recall_63: 0.9332\n",
      "Epoch 20/20\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.3956 - accuracy: 0.8858 - precision_63: 0.9170 - recall_63: 0.8569 - val_loss: 0.1913 - val_accuracy: 0.9437 - val_precision_63: 0.9550 - val_recall_63: 0.9348\n",
      "Epoch 1/20\n",
      "1080/1080 [==============================] - 13s 8ms/step - loss: 0.7433 - accuracy: 0.7759 - precision_64: 0.8796 - recall_64: 0.6836 - val_loss: 0.2403 - val_accuracy: 0.9255 - val_precision_64: 0.9438 - val_recall_64: 0.9122\n",
      "Epoch 2/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.3847 - accuracy: 0.8891 - precision_64: 0.9173 - recall_64: 0.8641 - val_loss: 0.2013 - val_accuracy: 0.9415 - val_precision_64: 0.9528 - val_recall_64: 0.9327\n",
      "Epoch 3/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.3313 - accuracy: 0.9050 - precision_64: 0.9270 - recall_64: 0.8868 - val_loss: 0.1630 - val_accuracy: 0.9497 - val_precision_64: 0.9584 - val_recall_64: 0.9440\n",
      "Epoch 4/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.3063 - accuracy: 0.9113 - precision_64: 0.9314 - recall_64: 0.8956 - val_loss: 0.1538 - val_accuracy: 0.9532 - val_precision_64: 0.9604 - val_recall_64: 0.9488\n",
      "Epoch 5/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2806 - accuracy: 0.9205 - precision_64: 0.9384 - recall_64: 0.9049 - val_loss: 0.1432 - val_accuracy: 0.9558 - val_precision_64: 0.9636 - val_recall_64: 0.9490\n",
      "Epoch 6/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2624 - accuracy: 0.9246 - precision_64: 0.9409 - recall_64: 0.9109 - val_loss: 0.1317 - val_accuracy: 0.9603 - val_precision_64: 0.9672 - val_recall_64: 0.9542\n",
      "Epoch 7/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2509 - accuracy: 0.9287 - precision_64: 0.9431 - recall_64: 0.9168 - val_loss: 0.1219 - val_accuracy: 0.9635 - val_precision_64: 0.9680 - val_recall_64: 0.9580\n",
      "Epoch 8/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2456 - accuracy: 0.9294 - precision_64: 0.9440 - recall_64: 0.9172 - val_loss: 0.1213 - val_accuracy: 0.9610 - val_precision_64: 0.9671 - val_recall_64: 0.9568\n",
      "Epoch 9/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2345 - accuracy: 0.9329 - precision_64: 0.9467 - recall_64: 0.9211 - val_loss: 0.1152 - val_accuracy: 0.9637 - val_precision_64: 0.9702 - val_recall_64: 0.9610\n",
      "Epoch 10/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2313 - accuracy: 0.9339 - precision_64: 0.9474 - recall_64: 0.9221 - val_loss: 0.0995 - val_accuracy: 0.9683 - val_precision_64: 0.9748 - val_recall_64: 0.9652\n",
      "Epoch 11/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2233 - accuracy: 0.9358 - precision_64: 0.9492 - recall_64: 0.9253 - val_loss: 0.1052 - val_accuracy: 0.9678 - val_precision_64: 0.9729 - val_recall_64: 0.9643\n",
      "Epoch 12/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2250 - accuracy: 0.9345 - precision_64: 0.9475 - recall_64: 0.9246 - val_loss: 0.1165 - val_accuracy: 0.9660 - val_precision_64: 0.9715 - val_recall_64: 0.9617\n",
      "Epoch 13/20\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 0.2195 - accuracy: 0.9374 - precision_64: 0.9502 - recall_64: 0.9265 - val_loss: 0.1129 - val_accuracy: 0.9638 - val_precision_64: 0.9700 - val_recall_64: 0.9595\n",
      "Epoch 14/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2101 - accuracy: 0.9389 - precision_64: 0.9519 - recall_64: 0.9287 - val_loss: 0.1151 - val_accuracy: 0.9638 - val_precision_64: 0.9704 - val_recall_64: 0.9605\n",
      "Epoch 15/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2059 - accuracy: 0.9392 - precision_64: 0.9508 - recall_64: 0.9296 - val_loss: 0.1040 - val_accuracy: 0.9677 - val_precision_64: 0.9729 - val_recall_64: 0.9640\n",
      "Epoch 16/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2044 - accuracy: 0.9400 - precision_64: 0.9521 - recall_64: 0.9305 - val_loss: 0.0844 - val_accuracy: 0.9757 - val_precision_64: 0.9801 - val_recall_64: 0.9710\n",
      "Epoch 17/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2024 - accuracy: 0.9423 - precision_64: 0.9528 - recall_64: 0.9327 - val_loss: 0.0932 - val_accuracy: 0.9715 - val_precision_64: 0.9760 - val_recall_64: 0.9688\n",
      "Epoch 18/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.2047 - accuracy: 0.9401 - precision_64: 0.9518 - recall_64: 0.9298 - val_loss: 0.0869 - val_accuracy: 0.9735 - val_precision_64: 0.9783 - val_recall_64: 0.9705\n",
      "Epoch 19/20\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 0.1976 - accuracy: 0.9422 - precision_64: 0.9530 - recall_64: 0.9330 - val_loss: 0.0905 - val_accuracy: 0.9733 - val_precision_64: 0.9767 - val_recall_64: 0.9707\n",
      "Epoch 20/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1940 - accuracy: 0.9438 - precision_64: 0.9550 - recall_64: 0.9353 - val_loss: 0.0874 - val_accuracy: 0.9730 - val_precision_64: 0.9772 - val_recall_64: 0.9698\n",
      "Epoch 1/20\n",
      "1080/1080 [==============================] - 10s 6ms/step - loss: 0.5057 - accuracy: 0.8459 - precision_65: 0.9038 - recall_65: 0.8004 - val_loss: 0.1764 - val_accuracy: 0.9435 - val_precision_65: 0.9534 - val_recall_65: 0.9348\n",
      "Epoch 2/20\n",
      "1080/1080 [==============================] - 6s 6ms/step - loss: 0.2684 - accuracy: 0.9208 - precision_65: 0.9378 - recall_65: 0.9072 - val_loss: 0.1405 - val_accuracy: 0.9542 - val_precision_65: 0.9617 - val_recall_65: 0.9497\n",
      "Epoch 3/20\n",
      "1080/1080 [==============================] - 5s 5ms/step - loss: 0.2243 - accuracy: 0.9320 - precision_65: 0.9455 - recall_65: 0.9210 - val_loss: 0.1180 - val_accuracy: 0.9643 - val_precision_65: 0.9695 - val_recall_65: 0.9605\n",
      "Epoch 4/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1953 - accuracy: 0.9426 - precision_65: 0.9528 - recall_65: 0.9332 - val_loss: 0.1001 - val_accuracy: 0.9677 - val_precision_65: 0.9726 - val_recall_65: 0.9645\n",
      "Epoch 5/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1792 - accuracy: 0.9458 - precision_65: 0.9558 - recall_65: 0.9381 - val_loss: 0.0906 - val_accuracy: 0.9728 - val_precision_65: 0.9766 - val_recall_65: 0.9687\n",
      "Epoch 6/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1709 - accuracy: 0.9480 - precision_65: 0.9583 - recall_65: 0.9406 - val_loss: 0.0827 - val_accuracy: 0.9738 - val_precision_65: 0.9767 - val_recall_65: 0.9710\n",
      "Epoch 7/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1578 - accuracy: 0.9529 - precision_65: 0.9614 - recall_65: 0.9461 - val_loss: 0.0808 - val_accuracy: 0.9745 - val_precision_65: 0.9780 - val_recall_65: 0.9718\n",
      "Epoch 8/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1512 - accuracy: 0.9540 - precision_65: 0.9620 - recall_65: 0.9474 - val_loss: 0.0753 - val_accuracy: 0.9770 - val_precision_65: 0.9802 - val_recall_65: 0.9747\n",
      "Epoch 9/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1449 - accuracy: 0.9562 - precision_65: 0.9634 - recall_65: 0.9505 - val_loss: 0.0514 - val_accuracy: 0.9845 - val_precision_65: 0.9871 - val_recall_65: 0.9825\n",
      "Epoch 10/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1374 - accuracy: 0.9584 - precision_65: 0.9656 - recall_65: 0.9528 - val_loss: 0.0584 - val_accuracy: 0.9822 - val_precision_65: 0.9844 - val_recall_65: 0.9807\n",
      "Epoch 11/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1329 - accuracy: 0.9590 - precision_65: 0.9654 - recall_65: 0.9533 - val_loss: 0.0530 - val_accuracy: 0.9828 - val_precision_65: 0.9853 - val_recall_65: 0.9815\n",
      "Epoch 12/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1246 - accuracy: 0.9624 - precision_65: 0.9690 - recall_65: 0.9575 - val_loss: 0.0610 - val_accuracy: 0.9780 - val_precision_65: 0.9819 - val_recall_65: 0.9772\n",
      "Epoch 13/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1223 - accuracy: 0.9627 - precision_65: 0.9687 - recall_65: 0.9579 - val_loss: 0.0480 - val_accuracy: 0.9840 - val_precision_65: 0.9868 - val_recall_65: 0.9815\n",
      "Epoch 14/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1201 - accuracy: 0.9638 - precision_65: 0.9699 - recall_65: 0.9591 - val_loss: 0.0553 - val_accuracy: 0.9835 - val_precision_65: 0.9864 - val_recall_65: 0.9800\n",
      "Epoch 15/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1189 - accuracy: 0.9638 - precision_65: 0.9694 - recall_65: 0.9590 - val_loss: 0.0579 - val_accuracy: 0.9823 - val_precision_65: 0.9838 - val_recall_65: 0.9797\n",
      "Epoch 16/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1115 - accuracy: 0.9657 - precision_65: 0.9710 - recall_65: 0.9614 - val_loss: 0.0498 - val_accuracy: 0.9843 - val_precision_65: 0.9873 - val_recall_65: 0.9832\n",
      "Epoch 17/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1099 - accuracy: 0.9656 - precision_65: 0.9709 - recall_65: 0.9611 - val_loss: 0.0439 - val_accuracy: 0.9887 - val_precision_65: 0.9901 - val_recall_65: 0.9875\n",
      "Epoch 18/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1083 - accuracy: 0.9667 - precision_65: 0.9718 - recall_65: 0.9626 - val_loss: 0.0401 - val_accuracy: 0.9872 - val_precision_65: 0.9900 - val_recall_65: 0.9860\n",
      "Epoch 19/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1063 - accuracy: 0.9678 - precision_65: 0.9722 - recall_65: 0.9636 - val_loss: 0.0336 - val_accuracy: 0.9897 - val_precision_65: 0.9911 - val_recall_65: 0.9888\n",
      "Epoch 20/20\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 0.1060 - accuracy: 0.9679 - precision_65: 0.9725 - recall_65: 0.9635 - val_loss: 0.0412 - val_accuracy: 0.9883 - val_precision_65: 0.9898 - val_recall_65: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# Choices for the Model Architecture - values arbitrary\n",
    "dense_layers = [6, 5, 4]\n",
    "layer_sizes = [16, 32, 64]\n",
    "conv_layers = [0]  # for right now, I don't want any convolutions\n",
    "\n",
    "# try different combinations!\n",
    "for num_fc_layer in dense_layers:\n",
    "    for size in layer_sizes:\n",
    "        for num_conv_layers in conv_layers:\n",
    "            # name the combo - based on it's architecture, and when it was run\n",
    "            NAME = (\n",
    "                f'{num_conv_layers}-conv-{size}-nodes' +\n",
    "                f'-{num_fc_layer}-dense_layers' + \n",
    "                f'-{int(time.time())}'\n",
    "            )\n",
    "            # Instantiate TensorBoard to visualize model performance\n",
    "            tensorboard = TensorBoard(log_dir=f'./Graph/{NAME}')\n",
    "            # Define Model\n",
    "            model = define_model(size, num_conv_layers, num_fc_layer, 0.2)\n",
    "            # Train the Model (using a generator!)\n",
    "            epochs, batch_size = 20, 20\n",
    "            history = train_model(model, train_ds_batches,\n",
    "                                  val_ds, epochs, [tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30aa9d",
   "metadata": {},
   "source": [
    "### Part C\n",
    "(30 pts) This assignment reflects the data collection process.\n",
    "- Hand write 5 styles of your own digits from 0 to 9 on a paper, and make sure your own handwritings are for sure recognizable by yourself. Please take a picture of each digit you write (so you have total 5 x 10 = 50 images), resize and convert it to the same data input format as the MNIST dataset. In other words, you have 50 new data points with labels.\n",
    "- Treat these 50 images as “brand-new” test dataset and run your own ML models from Question a and b on these 50 images and report the achieved test accuracy. Note, the goal for this exercise is not for achieving “high” accuracy, but to show what potential gaps there may be between existing MNIST dataset and your own test dataset, a scenario you would encounter in real life.\n",
    "- Use the following code for the image loading. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Photos\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "rootdir = \n",
    "\n",
    "def read_img(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    x_img = np.array([0] * 784)\n",
    "\n",
    "\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            idx = i * 28 + j\n",
    "            x_img[idx] = 255 - img[i][j][0]\n",
    "            \n",
    "    return x_img, img\n",
    "    \n",
    "X_mydigits = []\n",
    "Y_mydigits = []\n",
    "digit_imgs = []\n",
    "\n",
    "# read handwritten digits\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            path = os.path.join(subdir, file)\n",
    "            digit, img = read_img(path)\n",
    "            label = int(os.path.splitext(subdir)[0][-1])\n",
    "            X_mydigits.append(digit)\n",
    "            Y_mydigits.append(label)\n",
    "            digit_imgs.append(img)\n",
    "\n",
    "\n",
    "X_mydigits = np.array(X_mydigits)\n",
    "Y_mydigits = np.array(Y_mydigits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing all my images\n",
    "temp_img = ''\n",
    "for i in range(5):\n",
    "    hor = ''\n",
    "    for j in range(10):\n",
    "        idx = i*10 + j\n",
    "        img = digit_imgs[idx]\n",
    "        if j == 0:\n",
    "            hor = img\n",
    "        else:\n",
    "            hor = np.hstack((hor, img))\n",
    "    if i == 0:\n",
    "        temp_img = hor\n",
    "    else:\n",
    "        temp_img = np.vstack((temp_img, hor))\n",
    "\n",
    "plt.imshow(temp_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed5dd4",
   "metadata": {},
   "source": [
    "d.\tIn submission, the following is required for this assignment \n",
    "- Show all 50 images you create with the corresponding labels you intend to assign, \n",
    "- Make a table to show the test accuracy on these 50 images for each ML model you obtained from questions a to c.\n",
    "- Submit in a subfolder with your 50 handwriting dataset in MNIST format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863570c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('559')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "814950ec8b8c7032d23fac38b059fcb51ad391395255f22f9a4e55af65449f0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
