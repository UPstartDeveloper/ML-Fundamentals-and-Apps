{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS 559 HW 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhnq4YzdybKY"
      },
      "source": [
        "## Question 1 [ 40 Points ]\n",
        "\n",
        "**Support Vector Machines (SVMs)**\n",
        "\n",
        "[25 points ] Download this dataset, split it as a 80% training and 20% test set. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "# the link to the Iris dataset in the hw doc didn't work,\n",
        "# so I'll be importing through Scikit-learn directly\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "\n",
        "X = np.array(iris.data)\n",
        "y = np.array(iris.target).reshape(-1,1)\n",
        "iris_df = pd.DataFrame(data=np.column_stack([X, y]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4\n",
              "0  5.1  3.5  1.4  0.2  0.0\n",
              "1  4.9  3.0  1.4  0.2  0.0\n",
              "2  4.7  3.2  1.3  0.2  0.0\n",
              "3  4.6  3.1  1.5  0.2  0.0\n",
              "4  5.0  3.6  1.4  0.2  0.0"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "iris_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 1., 2.])"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "iris_df[4].unique() # how many classes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(quickly doing feature scaling before any further modeling development)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement the support vector algorithm from scratch using Numpy and Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "I49gdoFFyR7D"
      },
      "outputs": [],
      "source": [
        "class SVM:\n",
        "    \"\"\"\n",
        "    Credit to this presentation by Siraj Raval\n",
        "    for helping me on the code: \n",
        "    https://github.com/llSourcell/Classifying_Data_Using_a_Support_Vector_Machine/blob/master/support_vector_machine_lesson.ipynb\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Initialize our weight vector\n",
        "        self.parameters = (\n",
        "            list() # default - b/c we don't know how many features yet\n",
        "        )\n",
        "        self.bias = 0\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs=1000, regularization=1.0, learning_rate=0.005):\n",
        "        self.parameters = np.zeros((1, X_train.shape[1]))\n",
        "        # store accuraciesover each epoch so we can plot the learning curves later\n",
        "        accuracies = a = []\n",
        "\n",
        "        # train + optimize the model\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            error = 0\n",
        "            y_pred1 = self.predict(X_train)\n",
        "            y_pred2 = y_train * self.predict(X_train)\n",
        "            print(y_pred1)\n",
        "            # print(y_pred2)\n",
        "\n",
        "            for index, pred in enumerate(y_pred2):\n",
        "                # update rules\n",
        "                regularized_dW = -2 * (1/epoch) * self.parameters\n",
        "                if pred[0] < 1:  # misclassification\n",
        "                    self.parameters += learning_rate * (\n",
        "                        (X_train[index] * y_train[index]) + \n",
        "                        regularized_dW\n",
        "                    )\n",
        "                    error = 1\n",
        "                else: # correct classification\n",
        "                    self.parameters += (learning_rate * regularized_dW)\n",
        "            accuracies.append(metrics.accuracy_score(y_train, y_pred1))\n",
        "  \n",
        "    def predict(self, X):\n",
        "        return (np.dot(self.parameters, X.T) + self.bias).T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[10 points ] Report the accuracies for the train and test sets. Comment on whether your model has overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 4.31675704e-01]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 1.57794584e+00]\n",
            " [ 3.73058610e-01]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 1.21243997e+00]\n",
            " [ 2.40403582e-01]\n",
            " [ 5.19424963e-01]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 3.48501476e-02]\n",
            " [ 1.12440971e+00]\n",
            " [ 1.36476368e+00]\n",
            " [ 4.33385218e-01]\n",
            " [ 2.25182932e+00]\n",
            " [ 1.40049082e-01]\n",
            " [ 2.68731659e+00]\n",
            " [ 2.08702751e-01]\n",
            " [-0.00000000e+00]\n",
            " [ 2.81765868e+00]\n",
            " [ 9.53354387e-02]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [-1.90811955e-02]\n",
            " [ 1.34937318e+00]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 1.71430590e-01]\n",
            " [-0.00000000e+00]\n",
            " [ 7.18382026e-02]\n",
            " [ 2.37837433e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 2.96264945e-01]\n",
            " [ 1.31947384e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 9.70759408e-01]\n",
            " [ 1.21243997e+00]\n",
            " [ 2.83216915e-01]\n",
            " [ 1.52777789e-01]\n",
            " [ 1.42746372e+00]\n",
            " [ 2.29349171e-01]\n",
            " [-0.00000000e+00]\n",
            " [-1.40904094e-01]\n",
            " [ 1.61360357e+00]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 8.97924906e-02]\n",
            " [ 6.75385101e-01]\n",
            " [-0.00000000e+00]\n",
            " [ 1.53848483e+00]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 3.98073234e-01]\n",
            " [-1.82957090e-01]\n",
            " [ 2.03604718e+00]\n",
            " [ 5.46528290e-01]\n",
            " [ 1.21820457e+00]\n",
            " [ 2.73336252e+00]\n",
            " [ 1.70005182e-01]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 1.27989184e+00]\n",
            " [ 1.91685396e+00]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 6.18413921e-01]\n",
            " [ 1.62608029e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 1.87904082e+00]\n",
            " [ 1.89372971e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 3.72455181e-01]\n",
            " [ 5.36112526e-01]\n",
            " [ 1.20129731e+00]\n",
            " [ 5.35101526e-01]\n",
            " [ 1.62519471e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 1.84281488e+00]\n",
            " [ 2.43379820e-01]\n",
            " [ 1.53576742e+00]\n",
            " [-1.37579482e-03]\n",
            " [ 4.89091778e-02]\n",
            " [ 3.26379728e-01]\n",
            " [-0.00000000e+00]\n",
            " [ 4.57325243e-01]\n",
            " [ 1.67408134e-01]\n",
            " [-0.00000000e+00]\n",
            " [ 6.60111490e-02]\n",
            " [ 2.24923164e+00]\n",
            " [ 1.71069773e+00]\n",
            " [-0.00000000e+00]\n",
            " [-1.98538570e-01]\n",
            " [ 2.01900881e+00]\n",
            " [ 1.81577257e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 2.02781210e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 6.08156841e-01]\n",
            " [ 2.37596453e+00]\n",
            " [ 1.46033361e+00]\n",
            " [ 1.53317619e-01]\n",
            " [ 1.48003263e+00]\n",
            " [ 5.18662850e-01]\n",
            " [ 3.52189907e-01]\n",
            " [ 1.05270999e+00]\n",
            " [ 1.01325519e+00]\n",
            " [-0.00000000e+00]\n",
            " [ 2.48764085e-01]\n",
            " [ 4.76806252e-01]\n",
            " [-0.00000000e+00]\n",
            " [ 1.85681746e-01]\n",
            " [ 2.17804337e+00]]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/zainraza/Downloads/dev/courses/Stevens/CS-559/SyedRaza_HW4/HW_4.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zainraza/Downloads/dev/courses/Stevens/CS-559/SyedRaza_HW4/HW_4.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m custom_model \u001b[39m=\u001b[39m SVM()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zainraza/Downloads/dev/courses/Stevens/CS-559/SyedRaza_HW4/HW_4.ipynb#Y103sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m custom_model\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train)\n",
            "\u001b[1;32m/Users/zainraza/Downloads/dev/courses/Stevens/CS-559/SyedRaza_HW4/HW_4.ipynb Cell 14\u001b[0m in \u001b[0;36mSVM.fit\u001b[0;34m(self, X_train, y_train, epochs, regularization, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zainraza/Downloads/dev/courses/Stevens/CS-559/SyedRaza_HW4/HW_4.ipynb#Y103sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39melse\u001b[39;00m: \u001b[39m# correct classification\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zainraza/Downloads/dev/courses/Stevens/CS-559/SyedRaza_HW4/HW_4.ipynb#Y103sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (learning_rate \u001b[39m*\u001b[39m regularized_dW)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zainraza/Downloads/dev/courses/Stevens/CS-559/SyedRaza_HW4/HW_4.ipynb#Y103sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m accuracies\u001b[39m.\u001b[39mappend(metrics\u001b[39m.\u001b[39;49maccuracy_score(y_train, y_pred))\n",
            "File \u001b[0;32m/Applications/anaconda3/envs/559/lib/python3.10/site-packages/sklearn/metrics/_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[0;32m/Applications/anaconda3/envs/559/lib/python3.10/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
          ]
        }
      ],
      "source": [
        "custom_model = SVM()\n",
        "\n",
        "custom_model.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[5 points] Test your model performance with the scikit-learn model. Comment on the difference in accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INzXYmZWzrFg"
      },
      "outputs": [],
      "source": [
        "model = SVM()\n",
        "model.fit('Dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcdbdv0Sz3Xz"
      },
      "source": [
        "Training accuracy : \n",
        "\n",
        "Test accuracy : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES4uUFwXz-aX"
      },
      "source": [
        "## Question 2 [ 40 Points ]\n",
        "\n",
        "**Decision Trees**\n",
        "\n",
        "a. [5 points] Complete the `test_split` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "kDRZCShz45P3"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "FqI3dUn-z95m"
      },
      "outputs": [],
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\t# move all nodes that are true to the left, false to the right\n",
        "\t\tif row[index] >= value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:  # row[index] < value\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. [5 points] Complete the `gini_index` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "q338FeZs0p51"
      },
      "outputs": [],
      "source": [
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t\"\"\"\n",
        "\tUsing the calculation approach described here by Josh Gordon:\n",
        "\thttps://youtu.be/LDRbO9a6XPU?t=354\n",
        "\t\"\"\"\n",
        "\t# count all samples at split point\n",
        "\tleft_group, right_group = groups\n",
        "\tn_instances = len(left_group) + len(right_group)\n",
        "\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tif len(group) > 0:  # protect against ZeroDivisionError\n",
        "\t\t\tnum_classes_in_group = len(set(row[-1] for row in group))\n",
        "\t\t\tuncertainty = 1 - (1 / num_classes_in_group)\n",
        "\t\t\tgini += (len(group) / n_instances) * uncertainty\n",
        "\treturn gini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "c. [5 points] Complete the `get_split` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "PL6EsIaM05EJ"
      },
      "outputs": [],
      "source": [
        "def get_split(dataset):\n",
        "\t\"\"\"Find the best split point by iterating over every feature / value\n",
        "    and calculating the information gain.\"\"\"\n",
        "\tunique_labels = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\n",
        "\tfor feature_index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t\trow_val = row[feature_index]\n",
        "\t\t\tgroups = test_split(feature_index, row[feature_index], dataset) \n",
        "\t\t\tgini = gini_index(groups, unique_labels)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = (\n",
        "\t\t\t\t\tfeature_index, row_val, gini, groups\n",
        "\t\t\t\t)\n",
        "        \n",
        "\treturn {'index': b_index, 'value': b_value, 'groups': b_groups}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "cb_s3CsC1aGV"
      },
      "outputs": [],
      "source": [
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d. [15 points] Complete the `split` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "0Fv9McfA1cuP"
      },
      "outputs": [],
      "source": [
        "# Create child splits for a node or make terminal\n",
        "#Hint : Just call the to_terminal and get_split functions defined above. \n",
        "\n",
        "def split(node, max_depth, min_size, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        " \n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = (\n",
        "\t\t\tto_terminal(left), \n",
        "\t\t\tto_terminal(right)\n",
        "\t\t)\n",
        "\t\treturn\n",
        "\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(right)\n",
        "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "  \n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right)\n",
        "\t\tsplit(node['right'], max_depth, min_size, depth+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "KQiNy9NE2IoJ"
      },
      "outputs": [],
      "source": [
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "\troot = get_split(train)\n",
        "\tsplit(root, max_depth, min_size, 1)\n",
        "\treturn root"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "e. [10 points] Print the tree. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "rSYBf-DQ2LYs"
      },
      "outputs": [],
      "source": [
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "\tif isinstance(node, dict):\n",
        "\t\tprint('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "\t\tprint_tree(node['left'], depth+1)\n",
        "\t\tprint_tree(node['right'], depth+1)\n",
        "\telse:\n",
        "\t\tprint('%s[%s]' % ((depth*' ', node)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "MkuT-7SY40Yu"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "\n",
        "X = np.array(iris.data)\n",
        "y = np.array(iris.target).reshape(-1,1)\n",
        "\n",
        "data = np.append(X,y,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "42ivIbmh2j5Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[X3 < 3.000]\n",
            " [1.0]\n",
            " [0.0]\n"
          ]
        }
      ],
      "source": [
        "tree = build_tree(data, 1, 1)\n",
        "print_tree(tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAEFoJMP70So"
      },
      "source": [
        "## Question 3 [ 20 Points ]\n",
        "\n",
        "**Random Forests and Boosting**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_names = [\n",
        "    \"buying\", \"maint\", \"doors\",\n",
        "    \"persons\", \"lug_boot\", \"safety\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U1feyKIo73h5"
      },
      "outputs": [],
      "source": [
        "car_df = pd.read_csv(\n",
        "    './car/car.data',\n",
        "    sep=\",\", usecols=list(range(0, 6)), names=col_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>buying</th>\n",
              "      <th>maint</th>\n",
              "      <th>doors</th>\n",
              "      <th>persons</th>\n",
              "      <th>lug_boot</th>\n",
              "      <th>safety</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>small</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>small</td>\n",
              "      <td>med</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>small</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>med</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>med</td>\n",
              "      <td>med</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  buying  maint doors persons lug_boot safety\n",
              "0  vhigh  vhigh     2       2    small    low\n",
              "1  vhigh  vhigh     2       2    small    med\n",
              "2  vhigh  vhigh     2       2    small   high\n",
              "3  vhigh  vhigh     2       2      med    low\n",
              "4  vhigh  vhigh     2       2      med    med"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "car_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = col_names[:-1]\n",
        "target = [col_names[-1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing & Splitting Data \n",
        "\n",
        "The approach I'll go with here is that because all the features are *technically* categorical, I'll one-hot encode them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import metrics, model_selection\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_data = dict()\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "for feat in features:\n",
        "    input_feat = car_df[feat].values.reshape(-1, 1)\n",
        "    encoded_feat = enc.fit_transform(input_feat).toarray()\n",
        "    encoded_data[feat] = encoded_feat\n",
        "\n",
        "X = np.column_stack(list(\n",
        "    encoded_data.values()\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the target column, I'll merely use sparse integer encoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = car_df[\"safety\"].unique().tolist()\n",
        "convert_to_int = lambda label: classes.index(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = car_df[\"safety\"].transform(convert_to_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Comparing Ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First model - Random Forest coming right up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Train Accuracy: 100%\n",
            "Random Forest Test Accuracy: 28.3237%\n"
          ]
        }
      ],
      "source": [
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100, max_depth=1, \n",
        "    random_state=0, oob_score=True).fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Train Accuracy: {round(rf_clf.oob_score * 100, 4)}%\")\n",
        "print(f\"Random Forest Test Accuracy: {round(test_accuracy * 100, 4)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second model: Gradient Boosting, anyone?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosted Classifier Train Accuracy: 36.5412%\n",
            "Gradient Boosted Classifier Test Accuracy: 20.5202%\n"
          ]
        }
      ],
      "source": [
        "grad_boost_clf = GradientBoostingClassifier(\n",
        "    n_estimators=100, learning_rate=1.0,\n",
        "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
        "\n",
        "# get preds so we can eval accuracy\n",
        "y_pred = grad_boost_clf.predict(X_train)\n",
        "train_accuracy = metrics.accuracy_score(y_train, y_pred)\n",
        "print(f\"Gradient Boosted Classifier Train Accuracy: {round(train_accuracy * 100, 4)}%\")\n",
        "\n",
        "y_pred = grad_boost_clf.predict(X_test)\n",
        "test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(f\"Gradient Boosted Classifier Test Accuracy: {round(test_accuracy * 100, 4)}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see from above, the Random Forest classifier outperforms the Gradient Boosted by ~8% on the test dataset, when both were given the same 5 one-hot encoded features and hyperparameters for `n_estimators` and `max_depth`.\n",
        "\n",
        "Both model have higher train accuracies than their respective test accuracies, which suggests both have started overfitting. However, it appears to be much more severe in the case of the Random Forest, which has a perfect score for training accuracy (100%), vs. the ~36.54% for the Gradient Boosted tree."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "559",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "814950ec8b8c7032d23fac38b059fcb51ad391395255f22f9a4e55af65449f0a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
